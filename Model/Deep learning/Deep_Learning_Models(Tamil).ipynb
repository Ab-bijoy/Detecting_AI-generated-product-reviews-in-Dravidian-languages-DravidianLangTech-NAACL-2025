{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Import Libraries"
      ],
      "metadata": {
        "id": "J83bG6mUYHck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "id62jJeATivl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487d0f0a-a90c-4884-a5ca-5450629197c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install pandas scikit-learn torch nltk\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Train and Test Data"
      ],
      "metadata": {
        "id": "Vo1Iqma-YLmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df = pd.read_csv('final_augmented_data (tamil).csv')\n",
        "test_df = pd.read_csv('tam_training_data_hum_ai.csv')"
      ],
      "metadata": {
        "id": "Qu_2PIfoTqy8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop unnecessary columns and rename columns"
      ],
      "metadata": {
        "id": "w2rDKY4FYOR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df = train_df.drop(\"back_translated_DATA\", axis=1)\n",
        "train_df = train_df.rename(columns={\"back_translated_DATA_tamil\": \"DATA\"})\n"
      ],
      "metadata": {
        "id": "FkDp67RqTvsr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modify data"
      ],
      "metadata": {
        "id": "brDftQHbYTAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_index = train_df.columns.get_loc(\"ID\")\n",
        "cols = list(train_df.columns)\n",
        "cols.insert(id_index + 1, cols.pop(cols.index(\"DATA\")))\n",
        "train_df = train_df.loc[:, cols]"
      ],
      "metadata": {
        "id": "ijIAPEbMTzTl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode Labels"
      ],
      "metadata": {
        "id": "mUIfNWAWYYHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "le = LabelEncoder()\n",
        "train_df['LABEL'] = le.fit_transform(train_df['LABEL'])"
      ],
      "metadata": {
        "id": "rNM_8sMyUMqo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization and Vocabulary Building"
      ],
      "metadata": {
        "id": "gMqrJ1s3Ya_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_and_build_vocab(data, max_vocab_size=10000):\n",
        "    all_tokens = []\n",
        "    for text in data:\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    vocab_counter = Counter(all_tokens)\n",
        "    most_common = vocab_counter.most_common(max_vocab_size - 2)\n",
        "    vocab = {word: idx + 2 for idx, (word, _) in enumerate(most_common)}\n",
        "    vocab[\"<PAD>\"] = 0\n",
        "    vocab[\"<UNK>\"] = 1\n",
        "    return vocab\n",
        "\n",
        "def encode_texts(data, vocab, max_len=100):\n",
        "    encoded_texts = []\n",
        "    for text in data:\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        encoded = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
        "        if len(encoded) < max_len:\n",
        "            encoded += [vocab[\"<PAD>\"]] * (max_len - len(encoded))\n",
        "        else:\n",
        "            encoded = encoded[:max_len]\n",
        "        encoded_texts.append(encoded)\n",
        "    return np.array(encoded_texts)\n",
        "\n",
        "vocab = tokenize_and_build_vocab(train_df['DATA'])\n",
        "train_encoded = encode_texts(train_df['DATA'], vocab)\n",
        "train_labels = train_df['LABEL'].values"
      ],
      "metadata": {
        "id": "7GaTY37_UQj_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Dataset and DataLoader"
      ],
      "metadata": {
        "id": "9-6Ydo64YezV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = TextDataset(train_encoded, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "uSrSRUzMURcs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Model"
      ],
      "metadata": {
        "id": "akXSyzLyYhqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, hidden = self.rnn(embedded)\n",
        "        output = self.fc(hidden.squeeze(0))\n",
        "        return output\n",
        "\n",
        "# Model Configuration\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = len(le.classes_)\n",
        "model = RNNModel(vocab_size, embed_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "-cP984GXX-ZS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  LSTM Model"
      ],
      "metadata": {
        "id": "6MbWHZgXYkkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        output = self.fc(hidden.squeeze(0))\n",
        "        return output\n",
        "\n",
        "# Model Configuration\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = len(le.classes_)\n",
        "model = LSTMModel(vocab_size, embed_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "rXfev2LSX3ji"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU Model"
      ],
      "metadata": {
        "id": "N-4JSsoKYnO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, hidden = self.gru(embedded)\n",
        "        output = self.fc(hidden.squeeze(0))\n",
        "        return output\n",
        "\n",
        "# Model Configuration\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = len(le.classes_)\n",
        "model = GRUModel(vocab_size, embed_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "UhnqaX00XzJI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiLSTM Model"
      ],
      "metadata": {
        "id": "6X1Zs3ilYrf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BiLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.bilstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply hidden_dim by 2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.bilstm(embedded)\n",
        "        # Use the output of the last time step from both directions\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Model Configuration\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = len(le.classes_)\n",
        "model = BiLSTMModel(vocab_size, embed_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "jbPrFokvUlqf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Setup"
      ],
      "metadata": {
        "id": "eu7jwamwYveR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "G1m-dmMPWYk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c77f99dc-1405-4acf-ef81-e7e793a8f16a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTMModel(\n",
              "  (embedding): Embedding(3158, 100)\n",
              "  (bilstm): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "EAJzghuNYyOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "nZsMidMpWcQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5c0efc-64e8-4552-b117-f68958a6c4dd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Loss: 35.6072\n",
            "Epoch 2/500, Loss: 35.3979\n",
            "Epoch 3/500, Loss: 35.4961\n",
            "Epoch 4/500, Loss: 35.4473\n",
            "Epoch 5/500, Loss: 35.3714\n",
            "Epoch 6/500, Loss: 35.3943\n",
            "Epoch 7/500, Loss: 35.4244\n",
            "Epoch 8/500, Loss: 35.3754\n",
            "Epoch 9/500, Loss: 35.3672\n",
            "Epoch 10/500, Loss: 35.3575\n",
            "Epoch 11/500, Loss: 35.3583\n",
            "Epoch 12/500, Loss: 35.3717\n",
            "Epoch 13/500, Loss: 35.3646\n",
            "Epoch 14/500, Loss: 35.3595\n",
            "Epoch 15/500, Loss: 35.3580\n",
            "Epoch 16/500, Loss: 35.3596\n",
            "Epoch 17/500, Loss: 35.3600\n",
            "Epoch 18/500, Loss: 35.3637\n",
            "Epoch 19/500, Loss: 35.3712\n",
            "Epoch 20/500, Loss: 35.3626\n",
            "Epoch 21/500, Loss: 35.3572\n",
            "Epoch 22/500, Loss: 35.3561\n",
            "Epoch 23/500, Loss: 35.3753\n",
            "Epoch 24/500, Loss: 35.3629\n",
            "Epoch 25/500, Loss: 35.3544\n",
            "Epoch 26/500, Loss: 35.3556\n",
            "Epoch 27/500, Loss: 35.3634\n",
            "Epoch 28/500, Loss: 35.3546\n",
            "Epoch 29/500, Loss: 35.3602\n",
            "Epoch 30/500, Loss: 35.3659\n",
            "Epoch 31/500, Loss: 35.3566\n",
            "Epoch 32/500, Loss: 35.3622\n",
            "Epoch 33/500, Loss: 35.3597\n",
            "Epoch 34/500, Loss: 35.3618\n",
            "Epoch 35/500, Loss: 35.3590\n",
            "Epoch 36/500, Loss: 35.3562\n",
            "Epoch 37/500, Loss: 35.3543\n",
            "Epoch 38/500, Loss: 35.3588\n",
            "Epoch 39/500, Loss: 35.3598\n",
            "Epoch 40/500, Loss: 35.3610\n",
            "Epoch 41/500, Loss: 35.3529\n",
            "Epoch 42/500, Loss: 35.3555\n",
            "Epoch 43/500, Loss: 35.3568\n",
            "Epoch 44/500, Loss: 35.3618\n",
            "Epoch 45/500, Loss: 35.3554\n",
            "Epoch 46/500, Loss: 35.3568\n",
            "Epoch 47/500, Loss: 35.3523\n",
            "Epoch 48/500, Loss: 35.3550\n",
            "Epoch 49/500, Loss: 35.3551\n",
            "Epoch 50/500, Loss: 35.3554\n",
            "Epoch 51/500, Loss: 35.3543\n",
            "Epoch 52/500, Loss: 35.3545\n",
            "Epoch 53/500, Loss: 35.3573\n",
            "Epoch 54/500, Loss: 35.3569\n",
            "Epoch 55/500, Loss: 35.3562\n",
            "Epoch 56/500, Loss: 35.3547\n",
            "Epoch 57/500, Loss: 35.3613\n",
            "Epoch 58/500, Loss: 35.3535\n",
            "Epoch 59/500, Loss: 35.3534\n",
            "Epoch 60/500, Loss: 35.3532\n",
            "Epoch 61/500, Loss: 35.3542\n",
            "Epoch 62/500, Loss: 35.3656\n",
            "Epoch 63/500, Loss: 35.3545\n",
            "Epoch 64/500, Loss: 35.3546\n",
            "Epoch 65/500, Loss: 35.3565\n",
            "Epoch 66/500, Loss: 35.3525\n",
            "Epoch 67/500, Loss: 35.3546\n",
            "Epoch 68/500, Loss: 35.3541\n",
            "Epoch 69/500, Loss: 35.3535\n",
            "Epoch 70/500, Loss: 35.3526\n",
            "Epoch 71/500, Loss: 35.3574\n",
            "Epoch 72/500, Loss: 35.3574\n",
            "Epoch 73/500, Loss: 35.3517\n",
            "Epoch 74/500, Loss: 35.3599\n",
            "Epoch 75/500, Loss: 35.3582\n",
            "Epoch 76/500, Loss: 35.3547\n",
            "Epoch 77/500, Loss: 35.3591\n",
            "Epoch 78/500, Loss: 35.3560\n",
            "Epoch 79/500, Loss: 35.3546\n",
            "Epoch 80/500, Loss: 35.3591\n",
            "Epoch 81/500, Loss: 35.3531\n",
            "Epoch 82/500, Loss: 35.3541\n",
            "Epoch 83/500, Loss: 35.3531\n",
            "Epoch 84/500, Loss: 35.3537\n",
            "Epoch 85/500, Loss: 35.3519\n",
            "Epoch 86/500, Loss: 35.3565\n",
            "Epoch 87/500, Loss: 35.3553\n",
            "Epoch 88/500, Loss: 35.3549\n",
            "Epoch 89/500, Loss: 35.3543\n",
            "Epoch 90/500, Loss: 35.3573\n",
            "Epoch 91/500, Loss: 35.3550\n",
            "Epoch 92/500, Loss: 35.3555\n",
            "Epoch 93/500, Loss: 35.3526\n",
            "Epoch 94/500, Loss: 35.3511\n",
            "Epoch 95/500, Loss: 35.3551\n",
            "Epoch 96/500, Loss: 35.3535\n",
            "Epoch 97/500, Loss: 35.3529\n",
            "Epoch 98/500, Loss: 35.3535\n",
            "Epoch 99/500, Loss: 35.3533\n",
            "Epoch 100/500, Loss: 35.3564\n",
            "Epoch 101/500, Loss: 35.3539\n",
            "Epoch 102/500, Loss: 35.3529\n",
            "Epoch 103/500, Loss: 35.3542\n",
            "Epoch 104/500, Loss: 35.3542\n",
            "Epoch 105/500, Loss: 35.3522\n",
            "Epoch 106/500, Loss: 35.3516\n",
            "Epoch 107/500, Loss: 35.3537\n",
            "Epoch 108/500, Loss: 35.3512\n",
            "Epoch 109/500, Loss: 35.3543\n",
            "Epoch 110/500, Loss: 35.3595\n",
            "Epoch 111/500, Loss: 35.3512\n",
            "Epoch 112/500, Loss: 35.3519\n",
            "Epoch 113/500, Loss: 35.3532\n",
            "Epoch 114/500, Loss: 35.3561\n",
            "Epoch 115/500, Loss: 35.3509\n",
            "Epoch 116/500, Loss: 35.3524\n",
            "Epoch 117/500, Loss: 35.3514\n",
            "Epoch 118/500, Loss: 35.3524\n",
            "Epoch 119/500, Loss: 35.3536\n",
            "Epoch 120/500, Loss: 35.3543\n",
            "Epoch 121/500, Loss: 35.3533\n",
            "Epoch 122/500, Loss: 35.3547\n",
            "Epoch 123/500, Loss: 35.3559\n",
            "Epoch 124/500, Loss: 35.3525\n",
            "Epoch 125/500, Loss: 35.3579\n",
            "Epoch 126/500, Loss: 35.3527\n",
            "Epoch 127/500, Loss: 35.3527\n",
            "Epoch 128/500, Loss: 35.3547\n",
            "Epoch 129/500, Loss: 35.3545\n",
            "Epoch 130/500, Loss: 35.3539\n",
            "Epoch 131/500, Loss: 35.3522\n",
            "Epoch 132/500, Loss: 35.3533\n",
            "Epoch 133/500, Loss: 35.3533\n",
            "Epoch 134/500, Loss: 35.3532\n",
            "Epoch 135/500, Loss: 35.3528\n",
            "Epoch 136/500, Loss: 35.3546\n",
            "Epoch 137/500, Loss: 35.3543\n",
            "Epoch 138/500, Loss: 35.3591\n",
            "Epoch 139/500, Loss: 35.3552\n",
            "Epoch 140/500, Loss: 35.3552\n",
            "Epoch 141/500, Loss: 35.3531\n",
            "Epoch 142/500, Loss: 35.3527\n",
            "Epoch 143/500, Loss: 35.3528\n",
            "Epoch 144/500, Loss: 35.3528\n",
            "Epoch 145/500, Loss: 35.3521\n",
            "Epoch 146/500, Loss: 35.3544\n",
            "Epoch 147/500, Loss: 35.3575\n",
            "Epoch 148/500, Loss: 35.3542\n",
            "Epoch 149/500, Loss: 35.3558\n",
            "Epoch 150/500, Loss: 35.3525\n",
            "Epoch 151/500, Loss: 35.3539\n",
            "Epoch 152/500, Loss: 35.3532\n",
            "Epoch 153/500, Loss: 35.3543\n",
            "Epoch 154/500, Loss: 35.3526\n",
            "Epoch 155/500, Loss: 35.3537\n",
            "Epoch 156/500, Loss: 35.3526\n",
            "Epoch 157/500, Loss: 35.3537\n",
            "Epoch 158/500, Loss: 35.3518\n",
            "Epoch 159/500, Loss: 35.3538\n",
            "Epoch 160/500, Loss: 35.3534\n",
            "Epoch 161/500, Loss: 35.3545\n",
            "Epoch 162/500, Loss: 35.3517\n",
            "Epoch 163/500, Loss: 35.3543\n",
            "Epoch 164/500, Loss: 35.3537\n",
            "Epoch 165/500, Loss: 35.3517\n",
            "Epoch 166/500, Loss: 35.3519\n",
            "Epoch 167/500, Loss: 35.3530\n",
            "Epoch 168/500, Loss: 35.3514\n",
            "Epoch 169/500, Loss: 35.3524\n",
            "Epoch 170/500, Loss: 35.3539\n",
            "Epoch 171/500, Loss: 35.3526\n",
            "Epoch 172/500, Loss: 35.3536\n",
            "Epoch 173/500, Loss: 35.3524\n",
            "Epoch 174/500, Loss: 35.3521\n",
            "Epoch 175/500, Loss: 35.3523\n",
            "Epoch 176/500, Loss: 35.3540\n",
            "Epoch 177/500, Loss: 35.3529\n",
            "Epoch 178/500, Loss: 35.3524\n",
            "Epoch 179/500, Loss: 35.3519\n",
            "Epoch 180/500, Loss: 35.3541\n",
            "Epoch 181/500, Loss: 35.3515\n",
            "Epoch 182/500, Loss: 35.3526\n",
            "Epoch 183/500, Loss: 35.3522\n",
            "Epoch 184/500, Loss: 35.3580\n",
            "Epoch 185/500, Loss: 35.3512\n",
            "Epoch 186/500, Loss: 35.3518\n",
            "Epoch 187/500, Loss: 35.3535\n",
            "Epoch 188/500, Loss: 35.3524\n",
            "Epoch 189/500, Loss: 35.3542\n",
            "Epoch 190/500, Loss: 35.3531\n",
            "Epoch 191/500, Loss: 35.3540\n",
            "Epoch 192/500, Loss: 35.3528\n",
            "Epoch 193/500, Loss: 35.3537\n",
            "Epoch 194/500, Loss: 35.3530\n",
            "Epoch 195/500, Loss: 35.3579\n",
            "Epoch 196/500, Loss: 35.3517\n",
            "Epoch 197/500, Loss: 35.3573\n",
            "Epoch 198/500, Loss: 35.3515\n",
            "Epoch 199/500, Loss: 35.3518\n",
            "Epoch 200/500, Loss: 35.3534\n",
            "Epoch 201/500, Loss: 35.3517\n",
            "Epoch 202/500, Loss: 35.3527\n",
            "Epoch 203/500, Loss: 35.3559\n",
            "Epoch 204/500, Loss: 35.3515\n",
            "Epoch 205/500, Loss: 35.3536\n",
            "Epoch 206/500, Loss: 35.3527\n",
            "Epoch 207/500, Loss: 35.3522\n",
            "Epoch 208/500, Loss: 35.3606\n",
            "Epoch 209/500, Loss: 35.3518\n",
            "Epoch 210/500, Loss: 35.3518\n",
            "Epoch 211/500, Loss: 35.3531\n",
            "Epoch 212/500, Loss: 35.3526\n",
            "Epoch 213/500, Loss: 35.3526\n",
            "Epoch 214/500, Loss: 35.3513\n",
            "Epoch 215/500, Loss: 35.3562\n",
            "Epoch 216/500, Loss: 35.3528\n",
            "Epoch 217/500, Loss: 35.3531\n",
            "Epoch 218/500, Loss: 35.3531\n",
            "Epoch 219/500, Loss: 35.3538\n",
            "Epoch 220/500, Loss: 35.3531\n",
            "Epoch 221/500, Loss: 35.3525\n",
            "Epoch 222/500, Loss: 35.3531\n",
            "Epoch 223/500, Loss: 35.3530\n",
            "Epoch 224/500, Loss: 35.3545\n",
            "Epoch 225/500, Loss: 35.3537\n",
            "Epoch 226/500, Loss: 35.3541\n",
            "Epoch 227/500, Loss: 35.3534\n",
            "Epoch 228/500, Loss: 35.3520\n",
            "Epoch 229/500, Loss: 35.3531\n",
            "Epoch 230/500, Loss: 35.3532\n",
            "Epoch 231/500, Loss: 35.3542\n",
            "Epoch 232/500, Loss: 35.3519\n",
            "Epoch 233/500, Loss: 35.3541\n",
            "Epoch 234/500, Loss: 35.3518\n",
            "Epoch 235/500, Loss: 35.3520\n",
            "Epoch 236/500, Loss: 35.3550\n",
            "Epoch 237/500, Loss: 35.3547\n",
            "Epoch 238/500, Loss: 35.3532\n",
            "Epoch 239/500, Loss: 35.3549\n",
            "Epoch 240/500, Loss: 35.3546\n",
            "Epoch 241/500, Loss: 35.3521\n",
            "Epoch 242/500, Loss: 35.3543\n",
            "Epoch 243/500, Loss: 35.3548\n",
            "Epoch 244/500, Loss: 35.3532\n",
            "Epoch 245/500, Loss: 35.3500\n",
            "Epoch 246/500, Loss: 35.3559\n",
            "Epoch 247/500, Loss: 35.3538\n",
            "Epoch 248/500, Loss: 35.3552\n",
            "Epoch 249/500, Loss: 35.3519\n",
            "Epoch 250/500, Loss: 35.3540\n",
            "Epoch 251/500, Loss: 35.3515\n",
            "Epoch 252/500, Loss: 35.3561\n",
            "Epoch 253/500, Loss: 35.3524\n",
            "Epoch 254/500, Loss: 35.3562\n",
            "Epoch 255/500, Loss: 35.3541\n",
            "Epoch 256/500, Loss: 35.3535\n",
            "Epoch 257/500, Loss: 35.3535\n",
            "Epoch 258/500, Loss: 35.3533\n",
            "Epoch 259/500, Loss: 35.3521\n",
            "Epoch 260/500, Loss: 35.3527\n",
            "Epoch 261/500, Loss: 35.3541\n",
            "Epoch 262/500, Loss: 35.3530\n",
            "Epoch 263/500, Loss: 35.3549\n",
            "Epoch 264/500, Loss: 35.3532\n",
            "Epoch 265/500, Loss: 35.3552\n",
            "Epoch 266/500, Loss: 35.3532\n",
            "Epoch 267/500, Loss: 35.3518\n",
            "Epoch 268/500, Loss: 35.3515\n",
            "Epoch 269/500, Loss: 35.3542\n",
            "Epoch 270/500, Loss: 35.3521\n",
            "Epoch 271/500, Loss: 35.3526\n",
            "Epoch 272/500, Loss: 35.3572\n",
            "Epoch 273/500, Loss: 35.3563\n",
            "Epoch 274/500, Loss: 35.3545\n",
            "Epoch 275/500, Loss: 35.3545\n",
            "Epoch 276/500, Loss: 35.3572\n",
            "Epoch 277/500, Loss: 35.3532\n",
            "Epoch 278/500, Loss: 35.3540\n",
            "Epoch 279/500, Loss: 35.3510\n",
            "Epoch 280/500, Loss: 35.3518\n",
            "Epoch 281/500, Loss: 35.3524\n",
            "Epoch 282/500, Loss: 35.3516\n",
            "Epoch 283/500, Loss: 35.3541\n",
            "Epoch 284/500, Loss: 35.3524\n",
            "Epoch 285/500, Loss: 35.3515\n",
            "Epoch 286/500, Loss: 35.3536\n",
            "Epoch 287/500, Loss: 35.3516\n",
            "Epoch 288/500, Loss: 35.3525\n",
            "Epoch 289/500, Loss: 35.3524\n",
            "Epoch 290/500, Loss: 35.3563\n",
            "Epoch 291/500, Loss: 35.3507\n",
            "Epoch 292/500, Loss: 35.3526\n",
            "Epoch 293/500, Loss: 35.3549\n",
            "Epoch 294/500, Loss: 35.3535\n",
            "Epoch 295/500, Loss: 35.3537\n",
            "Epoch 296/500, Loss: 35.3564\n",
            "Epoch 297/500, Loss: 35.3496\n",
            "Epoch 298/500, Loss: 35.3523\n",
            "Epoch 299/500, Loss: 35.3524\n",
            "Epoch 300/500, Loss: 35.3529\n",
            "Epoch 301/500, Loss: 35.3518\n",
            "Epoch 302/500, Loss: 35.3552\n",
            "Epoch 303/500, Loss: 35.3525\n",
            "Epoch 304/500, Loss: 35.3518\n",
            "Epoch 305/500, Loss: 35.3535\n",
            "Epoch 306/500, Loss: 35.3520\n",
            "Epoch 307/500, Loss: 35.3541\n",
            "Epoch 308/500, Loss: 35.3541\n",
            "Epoch 309/500, Loss: 35.3517\n",
            "Epoch 310/500, Loss: 35.3520\n",
            "Epoch 311/500, Loss: 35.3532\n",
            "Epoch 312/500, Loss: 35.3519\n",
            "Epoch 313/500, Loss: 35.3524\n",
            "Epoch 314/500, Loss: 35.3517\n",
            "Epoch 315/500, Loss: 35.3530\n",
            "Epoch 316/500, Loss: 35.3560\n",
            "Epoch 317/500, Loss: 35.3523\n",
            "Epoch 318/500, Loss: 35.3517\n",
            "Epoch 319/500, Loss: 35.3526\n",
            "Epoch 320/500, Loss: 35.3522\n",
            "Epoch 321/500, Loss: 35.3523\n",
            "Epoch 322/500, Loss: 35.3532\n",
            "Epoch 323/500, Loss: 35.3528\n",
            "Epoch 324/500, Loss: 35.3528\n",
            "Epoch 325/500, Loss: 35.3531\n",
            "Epoch 326/500, Loss: 35.3517\n",
            "Epoch 327/500, Loss: 35.3558\n",
            "Epoch 328/500, Loss: 35.3573\n",
            "Epoch 329/500, Loss: 35.3528\n",
            "Epoch 330/500, Loss: 35.3529\n",
            "Epoch 331/500, Loss: 35.3522\n",
            "Epoch 332/500, Loss: 35.3531\n",
            "Epoch 333/500, Loss: 35.3527\n",
            "Epoch 334/500, Loss: 35.3529\n",
            "Epoch 335/500, Loss: 35.3521\n",
            "Epoch 336/500, Loss: 35.3580\n",
            "Epoch 337/500, Loss: 35.3521\n",
            "Epoch 338/500, Loss: 35.3554\n",
            "Epoch 339/500, Loss: 35.3557\n",
            "Epoch 340/500, Loss: 35.3523\n",
            "Epoch 341/500, Loss: 35.3553\n",
            "Epoch 342/500, Loss: 35.3550\n",
            "Epoch 343/500, Loss: 35.3526\n",
            "Epoch 344/500, Loss: 35.3533\n",
            "Epoch 345/500, Loss: 35.3523\n",
            "Epoch 346/500, Loss: 35.3519\n",
            "Epoch 347/500, Loss: 35.3562\n",
            "Epoch 348/500, Loss: 35.3525\n",
            "Epoch 349/500, Loss: 35.3549\n",
            "Epoch 350/500, Loss: 35.3534\n",
            "Epoch 351/500, Loss: 35.3611\n",
            "Epoch 352/500, Loss: 35.3537\n",
            "Epoch 353/500, Loss: 35.3521\n",
            "Epoch 354/500, Loss: 35.3527\n",
            "Epoch 355/500, Loss: 35.3559\n",
            "Epoch 356/500, Loss: 35.3515\n",
            "Epoch 357/500, Loss: 35.3553\n",
            "Epoch 358/500, Loss: 35.3547\n",
            "Epoch 359/500, Loss: 35.3521\n",
            "Epoch 360/500, Loss: 35.3534\n",
            "Epoch 361/500, Loss: 35.3523\n",
            "Epoch 362/500, Loss: 35.3542\n",
            "Epoch 363/500, Loss: 35.3523\n",
            "Epoch 364/500, Loss: 35.3527\n",
            "Epoch 365/500, Loss: 35.3553\n",
            "Epoch 366/500, Loss: 35.3524\n",
            "Epoch 367/500, Loss: 35.3548\n",
            "Epoch 368/500, Loss: 35.3520\n",
            "Epoch 369/500, Loss: 35.3520\n",
            "Epoch 370/500, Loss: 35.3535\n",
            "Epoch 371/500, Loss: 35.3550\n",
            "Epoch 372/500, Loss: 35.3530\n",
            "Epoch 373/500, Loss: 35.3517\n",
            "Epoch 374/500, Loss: 35.3533\n",
            "Epoch 375/500, Loss: 35.3527\n",
            "Epoch 376/500, Loss: 35.3536\n",
            "Epoch 377/500, Loss: 35.3522\n",
            "Epoch 378/500, Loss: 35.3528\n",
            "Epoch 379/500, Loss: 35.3546\n",
            "Epoch 380/500, Loss: 35.3527\n",
            "Epoch 381/500, Loss: 35.3524\n",
            "Epoch 382/500, Loss: 35.3544\n",
            "Epoch 383/500, Loss: 35.3525\n",
            "Epoch 384/500, Loss: 35.3511\n",
            "Epoch 385/500, Loss: 35.3569\n",
            "Epoch 386/500, Loss: 35.3528\n",
            "Epoch 387/500, Loss: 35.3523\n",
            "Epoch 388/500, Loss: 35.3552\n",
            "Epoch 389/500, Loss: 35.3525\n",
            "Epoch 390/500, Loss: 35.3530\n",
            "Epoch 391/500, Loss: 35.3531\n",
            "Epoch 392/500, Loss: 35.3524\n",
            "Epoch 393/500, Loss: 35.3503\n",
            "Epoch 394/500, Loss: 35.3537\n",
            "Epoch 395/500, Loss: 35.3526\n",
            "Epoch 396/500, Loss: 35.3541\n",
            "Epoch 397/500, Loss: 35.3525\n",
            "Epoch 398/500, Loss: 35.3545\n",
            "Epoch 399/500, Loss: 35.3545\n",
            "Epoch 400/500, Loss: 35.3512\n",
            "Epoch 401/500, Loss: 35.3523\n",
            "Epoch 402/500, Loss: 35.3526\n",
            "Epoch 403/500, Loss: 35.3523\n",
            "Epoch 404/500, Loss: 35.3514\n",
            "Epoch 405/500, Loss: 35.3530\n",
            "Epoch 406/500, Loss: 35.3535\n",
            "Epoch 407/500, Loss: 35.3524\n",
            "Epoch 408/500, Loss: 35.3526\n",
            "Epoch 409/500, Loss: 35.3541\n",
            "Epoch 410/500, Loss: 35.3546\n",
            "Epoch 411/500, Loss: 35.3535\n",
            "Epoch 412/500, Loss: 35.3515\n",
            "Epoch 413/500, Loss: 35.3524\n",
            "Epoch 414/500, Loss: 35.3558\n",
            "Epoch 415/500, Loss: 35.3533\n",
            "Epoch 416/500, Loss: 35.3536\n",
            "Epoch 417/500, Loss: 35.3551\n",
            "Epoch 418/500, Loss: 35.3533\n",
            "Epoch 419/500, Loss: 35.3521\n",
            "Epoch 420/500, Loss: 35.3517\n",
            "Epoch 421/500, Loss: 35.3530\n",
            "Epoch 422/500, Loss: 35.3515\n",
            "Epoch 423/500, Loss: 35.3529\n",
            "Epoch 424/500, Loss: 35.3541\n",
            "Epoch 425/500, Loss: 35.3587\n",
            "Epoch 426/500, Loss: 35.3524\n",
            "Epoch 427/500, Loss: 35.3536\n",
            "Epoch 428/500, Loss: 35.3522\n",
            "Epoch 429/500, Loss: 35.3570\n",
            "Epoch 430/500, Loss: 35.3539\n",
            "Epoch 431/500, Loss: 35.3521\n",
            "Epoch 432/500, Loss: 35.3555\n",
            "Epoch 433/500, Loss: 35.3514\n",
            "Epoch 434/500, Loss: 35.3522\n",
            "Epoch 435/500, Loss: 35.3514\n",
            "Epoch 436/500, Loss: 35.3533\n",
            "Epoch 437/500, Loss: 35.3520\n",
            "Epoch 438/500, Loss: 35.3523\n",
            "Epoch 439/500, Loss: 35.3531\n",
            "Epoch 440/500, Loss: 35.3533\n",
            "Epoch 441/500, Loss: 35.3532\n",
            "Epoch 442/500, Loss: 35.3541\n",
            "Epoch 443/500, Loss: 35.3538\n",
            "Epoch 444/500, Loss: 35.3523\n",
            "Epoch 445/500, Loss: 35.3569\n",
            "Epoch 446/500, Loss: 35.3549\n",
            "Epoch 447/500, Loss: 35.3518\n",
            "Epoch 448/500, Loss: 35.3553\n",
            "Epoch 449/500, Loss: 35.3556\n",
            "Epoch 450/500, Loss: 35.3518\n",
            "Epoch 451/500, Loss: 35.3523\n",
            "Epoch 452/500, Loss: 35.3534\n",
            "Epoch 453/500, Loss: 35.3529\n",
            "Epoch 454/500, Loss: 35.3540\n",
            "Epoch 455/500, Loss: 35.3552\n",
            "Epoch 456/500, Loss: 35.3523\n",
            "Epoch 457/500, Loss: 35.3542\n",
            "Epoch 458/500, Loss: 35.3520\n",
            "Epoch 459/500, Loss: 35.3562\n",
            "Epoch 460/500, Loss: 35.3561\n",
            "Epoch 461/500, Loss: 35.3561\n",
            "Epoch 462/500, Loss: 35.3509\n",
            "Epoch 463/500, Loss: 35.3528\n",
            "Epoch 464/500, Loss: 35.3521\n",
            "Epoch 465/500, Loss: 35.3543\n",
            "Epoch 466/500, Loss: 35.3536\n",
            "Epoch 467/500, Loss: 35.3519\n",
            "Epoch 468/500, Loss: 35.3546\n",
            "Epoch 469/500, Loss: 35.3544\n",
            "Epoch 470/500, Loss: 35.3537\n",
            "Epoch 471/500, Loss: 35.3533\n",
            "Epoch 472/500, Loss: 35.3512\n",
            "Epoch 473/500, Loss: 35.3529\n",
            "Epoch 474/500, Loss: 35.3529\n",
            "Epoch 475/500, Loss: 35.3610\n",
            "Epoch 476/500, Loss: 35.3512\n",
            "Epoch 477/500, Loss: 35.3560\n",
            "Epoch 478/500, Loss: 35.3561\n",
            "Epoch 479/500, Loss: 35.3544\n",
            "Epoch 480/500, Loss: 35.3536\n",
            "Epoch 481/500, Loss: 35.3528\n",
            "Epoch 482/500, Loss: 35.3524\n",
            "Epoch 483/500, Loss: 35.3537\n",
            "Epoch 484/500, Loss: 35.3515\n",
            "Epoch 485/500, Loss: 35.3557\n",
            "Epoch 486/500, Loss: 35.3556\n",
            "Epoch 487/500, Loss: 35.3517\n",
            "Epoch 488/500, Loss: 35.3544\n",
            "Epoch 489/500, Loss: 35.3527\n",
            "Epoch 490/500, Loss: 35.3519\n",
            "Epoch 491/500, Loss: 35.3528\n",
            "Epoch 492/500, Loss: 35.3516\n",
            "Epoch 493/500, Loss: 35.3549\n",
            "Epoch 494/500, Loss: 35.3520\n",
            "Epoch 495/500, Loss: 35.3527\n",
            "Epoch 496/500, Loss: 35.3546\n",
            "Epoch 497/500, Loss: 35.3536\n",
            "Epoch 498/500, Loss: 35.3533\n",
            "Epoch 499/500, Loss: 35.3523\n",
            "Epoch 500/500, Loss: 35.3539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Test Data"
      ],
      "metadata": {
        "id": "___D3IZ5Y0c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_df.dropna(subset=['DATA'], inplace=True)\n",
        "test_encoded = encode_texts(test_df['DATA'], vocab)\n",
        "test_dataset = TextDataset(test_encoded, np.zeros(len(test_encoded)))\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "x9ga-C27Wfus"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Labels"
      ],
      "metadata": {
        "id": "bPlDepoNY2bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for texts, _ in test_loader:\n",
        "        texts = texts.to(device)\n",
        "        outputs = model(texts)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "\n",
        "decoded_labels = le.inverse_transform(all_preds)\n",
        "test_df['LABEL'] = decoded_labels"
      ],
      "metadata": {
        "id": "TX6O0vp5WiT2"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using BiLSTM**"
      ],
      "metadata": {
        "id": "NoG2adkMYv0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the actual labels from the Excel file\n",
        "actual_labels = pd.read_csv('tamil-test.xlsx - Sheet1.csv')['Label'].values\n",
        "\n",
        "# Ensure both sets of labels are the same length\n",
        "min_len = min(len(actual_labels), len(decoded_labels))\n",
        "actual_labels = actual_labels[:min_len]\n",
        "decoded_labels = decoded_labels[:min_len]\n",
        "\n",
        "\n",
        "# Calculate the metrics\n",
        "accuracy = accuracy_score(actual_labels, decoded_labels)\n",
        "precision = precision_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "recall = recall_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "f1 = f1_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCpkeydyYXK3",
        "outputId": "c4211490-0ac6-410e-9720-bbc422d49d80"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.48\n",
            "Precision: 0.2304\n",
            "Recall: 0.48\n",
            "F1 Score: 0.3113513513513514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UIrINJmOYVEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using GRU**"
      ],
      "metadata": {
        "id": "wnZ5CqcVYKMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the actual labels from the Excel file\n",
        "actual_labels = pd.read_csv('tamil-test.xlsx - Sheet1.csv')['Label'].values\n",
        "\n",
        "# Ensure both sets of labels are the same length\n",
        "min_len = min(len(actual_labels), len(decoded_labels))\n",
        "actual_labels = actual_labels[:min_len]\n",
        "decoded_labels = decoded_labels[:min_len]\n",
        "\n",
        "\n",
        "# Calculate the metrics\n",
        "accuracy = accuracy_score(actual_labels, decoded_labels)\n",
        "precision = precision_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "recall = recall_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "f1 = f1_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waMRtAKwX1J7",
        "outputId": "4c78fd82-ae0d-4576-89cb-3e85a05204ea"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.51\n",
            "Precision: 0.4602105263157895\n",
            "Recall: 0.51\n",
            "F1 Score: 0.3828930817610063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using LSTM**"
      ],
      "metadata": {
        "id": "DzCyPjlPXXDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the actual labels from the Excel file\n",
        "actual_labels = pd.read_csv('tamil-test.xlsx - Sheet1.csv')['Label'].values\n",
        "\n",
        "# Ensure both sets of labels are the same length\n",
        "min_len = min(len(actual_labels), len(decoded_labels))\n",
        "actual_labels = actual_labels[:min_len]\n",
        "decoded_labels = decoded_labels[:min_len]\n",
        "\n",
        "\n",
        "# Calculate the metrics\n",
        "accuracy = accuracy_score(actual_labels, decoded_labels)\n",
        "precision = precision_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "recall = recall_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "f1 = f1_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxDN9zrsWnkx",
        "outputId": "999bc12f-d866-4058-9656-60f91940e844"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.48\n",
            "Precision: 0.2304\n",
            "Recall: 0.48\n",
            "F1 Score: 0.3113513513513514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using RNN Model**"
      ],
      "metadata": {
        "id": "R6sUvKJ5WVI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: compare the labels of the mal_test.xlsx - Sheet1.csv with the predicted labels and Accuracy,Precision,Recall,F1 Score\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the actual labels from the Excel file\n",
        "actual_labels = pd.read_csv('tamil-test.xlsx - Sheet1.csv')['Label'].values\n",
        "\n",
        "# Ensure both sets of labels are the same length\n",
        "min_len = min(len(actual_labels), len(decoded_labels))\n",
        "actual_labels = actual_labels[:min_len]\n",
        "decoded_labels = decoded_labels[:min_len]\n",
        "\n",
        "\n",
        "# Calculate the metrics\n",
        "accuracy = accuracy_score(actual_labels, decoded_labels)\n",
        "precision = precision_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "recall = recall_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "f1 = f1_score(actual_labels, decoded_labels, average='weighted') # Use weighted for multiclass\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2_IzqPYU1OE",
        "outputId": "96bd67ca-644a-4c6d-a96b-0f85caa14d7d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.52\n",
            "Precision: 0.2704\n",
            "Recall: 0.52\n",
            "F1 Score: 0.35578947368421054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make tsv file"
      ],
      "metadata": {
        "id": "LGw1rlZaY4Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_df.to_csv('Team_Absolute_Zero_mal_bilstm.tsv', sep='\\t', index=False)\n",
        "print(\"Predictions saved to 'Team_Absolute_Zero_mal_bilstm.tsv'\")"
      ],
      "metadata": {
        "id": "8tSb8ltYW9XG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}